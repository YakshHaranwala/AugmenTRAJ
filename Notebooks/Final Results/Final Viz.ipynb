{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/nicholasjesperson/Documents/School/Comp4780/Data_Augmentation/Data Augmentation/paper/AugmenTRAJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score, accuracy_score, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from ptrail.core.Datasets import Datasets\n",
    "from ptrail.preprocessing.filters import Filters\n",
    "from ptrail.core.TrajectoryDF import PTRAILDataFrame\n",
    "from ptrail.preprocessing.statistics import Statistics\n",
    "\n",
    "from src.augmentation.augment import Augmentation\n",
    "from src.selection.select import Selection\n",
    "from src.utils.alter import Alter\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from random import Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def trajectoryAugumentationProcedure(trajs, seed, n, k, pradius, class_name, selection, augment):\n",
    "    myRandom = Random(seed * (n * k * pradius))\n",
    "\n",
    "    # Select the trajectories and remove duplicates from original dataset.\n",
    "    if selection == 'random':\n",
    "        splits = Selection.select_randomly(trajs, .2)\n",
    "    elif selection == 'fewest':\n",
    "        splits = Selection.select_traj_with_fewest(trajs, myRandom, .2)\n",
    "    else:\n",
    "        splits = Selection.select_representative_trajectories(trajs, class_name)\n",
    "    paramTestingDataSet = Filters.remove_duplicates(dataframe=trajs)\n",
    "\n",
    "    trainDataParm = paramTestingDataSet.loc[paramTestingDataSet.traj_id.isin(splits[\"train\"]) == True].dropna()\n",
    "    testDataParm = paramTestingDataSet.loc[paramTestingDataSet.traj_id.isin(splits[\"test\"]) == True].dropna()\n",
    "    testData = PTRAILDataFrame(data_set=testDataParm,\n",
    "                               latitude='lat',\n",
    "                               longitude='lon',\n",
    "                               datetime='DateTime',\n",
    "                               traj_id='traj_id')\n",
    "\n",
    "\n",
    "    noiseTraj = trainDataParm[class_name].unique()\n",
    "\n",
    "    sampledTraj = myRandom.choices(sorted(noiseTraj), k=math.floor(n * len(noiseTraj)))\n",
    "    for traj in sampledTraj:\n",
    "        trajToChange = trainDataParm.loc[trainDataParm.traj_id == traj]\n",
    "\n",
    "        #Trajectory must be changed\n",
    "        if augment == 'on':\n",
    "            trajChanged = Augmentation.augment_trajectories_with_randomly_generated_points(trajToChange, pradius,\n",
    "                                                                                         k, 100, myRandom, 'on')\n",
    "        elif augment == 'in':\n",
    "            trajChanged = Augmentation.augment_trajectories_with_randomly_generated_points(trajToChange, pradius,\n",
    "                                                                                          k, 100, myRandom, 'in')\n",
    "        else:\n",
    "            trajChanged = Augmentation.augment_trajectories_with_interpolation(trajToChange, 3600*4, 'linear')\n",
    "        trainDataParm = pd.concat([trainDataParm, trajChanged], ignore_index = True)\n",
    "\n",
    "    trainDataNoise = PTRAILDataFrame(data_set=trainDataParm,\n",
    "                                            datetime='DateTime',\n",
    "                                            traj_id='traj_id',\n",
    "                                            latitude='lat',\n",
    "                                            longitude='lon')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return testData, trainDataNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Dataset Facts ------------------------------\n",
      "\n",
      "Number of unique Trajectories in the data: 253\n",
      "Number of points in the data: 287136\n",
      "Dataset time range: 1196 days 22:51:45\n",
      "Datatype of the DataFrame: <class 'ptrail.core.TrajectoryDF.PTRAILDataFrame'>\n",
      "Dataset Bounding Box: (45.18896978643169, -118.61020848239596, 45.314545642992, -118.50455596234036)\n",
      "\n",
      "---------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the hurricane dataset.\n",
    "original_hurricane = Datasets.load_starkey()\n",
    "stats = Statistics.generate_kinematic_stats(original_hurricane, \"Species\")\n",
    "pivoted_original_stats = Statistics.pivot_stats_df(stats, \"Species\")\n",
    "\n",
    "# Read the results from the file.\n",
    "all_hurricane_results = pd.read_csv('./FinalStarkeyResults.csv', sep=',', header=None)\n",
    "all_hurricane_results.columns = ['seed', 'n_val', 'k_val', 'rad_val', 'traj_selection_method', 'augmentation_method', 'f1_score']\n",
    "all_hurricane_results.sort_values(by='f1_score', ascending=False, ignore_index=True, inplace=True)\n",
    "best_starkey_result_set = all_hurricane_results.iloc[0, :].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "testData, trainingData = trajectoryAugumentationProcedure(original_hurricane, best_starkey_result_set[0],\n",
    "                                                        best_starkey_result_set[1], best_starkey_result_set[2],\n",
    "                                                        best_starkey_result_set[3], 'Species',\n",
    "                                                        best_starkey_result_set[4], best_starkey_result_set[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_original_dataset(ax, target_col):\n",
    "    ax[0].set_title(\"1. Starkey data set\")\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    ogX = scaler.fit_transform(pivoted_original_stats.drop(columns=[target_col]))\n",
    "    ogY = pivoted_original_stats[target_col]\n",
    "    # Plot all points of the dataset.\n",
    "    pca = PCA(n_components=2)\n",
    "    Xt = pca.fit_transform(X=ogX)\n",
    "    h = 0.05\n",
    "    x_min, x_max = Xt[:, 0].min() - .5, Xt[:, 0].max() + .5\n",
    "    y_min, y_max = Xt[:, 1].min() - .5, Xt[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    sns.scatterplot(x=Xt[:, 0], y=Xt[:, 1],\n",
    "                      hue=ogY, ax=ax[0], palette=['red', 'yellow', 'royalblue'])\n",
    "    ax[0].set_xlim(xx.min(), xx.max())\n",
    "    ax[0].set_ylim(yy.min(), yy.max())\n",
    "    handles, labels  =  ax[0].get_legend_handles_labels()\n",
    "    ax[0].legend(handles, ['Elk', 'Deer', 'Cattle'], loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "seed = 34\n",
    "\n",
    "def plot_contours_original(dataset, ax, title, target_col, i):\n",
    "    model = RandomForestClassifier(random_state=best_starkey_result_set[0])\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X = scaler.fit_transform(dataset.drop(columns=[target_col]))\n",
    "    Y = dataset[target_col]\n",
    "\n",
    "    # Perform PCA on our df and extract 2 components for visualization purposes.\n",
    "    pca = PCA(n_components=2)\n",
    "    Xt = pca.fit_transform(X=X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xt, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "    h = 0.05\n",
    "    x_min, x_max = Xt[:, 0].min() - .5, Xt[:, 0].max() + .5\n",
    "    y_min, y_max = Xt[:, 1].min() - .5, Xt[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    # cm_bright = ListedColormap([\"#FF0000\", \"#FFFF00\", \"#00FF00\"])\n",
    "    cm = plt.cm.RdYlBu\n",
    "\n",
    "    # for i, name, model in zip(range(1, len(ax)), names, models):\n",
    "    model.fit(X_train, y_train)\n",
    "    hue = model.predict(X_test)\n",
    "    score = f1_score(y_test, hue, average='weighted')\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Reshape the array and then plot the contour plot.\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax[i].contourf(xx, yy, Z, cmap=cm, alpha=0.75)\n",
    "\n",
    "    # Now, we plot the points onto the contour and then map their\n",
    "    # colors according to the regions.\n",
    "    sns.scatterplot(x=X_test[:, 0], y=X_test[:, 1],\n",
    "                  hue=y_test, ax=ax[i], palette=['red', 'yellow', 'royalblue'])\n",
    "\n",
    "    ax[i].set_title(f'{title}, F-Score: {round(score, 2)}')\n",
    "    ax[i].get_legend().remove()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_contour_augmented(testData, trainingData, ax, title, i, class_name):\n",
    "    model = RandomForestClassifier(random_state=best_starkey_result_set[0])\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    pca = PCA(n_components=2)\n",
    "\n",
    "    \n",
    "    statsTestParm = Statistics.generate_kinematic_stats(dataframe=testData, target_col_name=class_name)\n",
    "    pivotedStatsTestParm = Statistics.pivot_stats_df(dataframe=statsTestParm, target_col_name=class_name)\n",
    "    pivotedStatsTestParm = pivotedStatsTestParm.loc[:,~pivotedStatsTestParm.columns.duplicated()]\n",
    "    testX = pivotedStatsTestParm.drop(columns=class_name)\n",
    "    testY = pivotedStatsTestParm[class_name].to_numpy()\n",
    "    \n",
    "    \n",
    "    statsTrainNoiseParm = Statistics.generate_kinematic_stats(dataframe=trainingData, target_col_name=class_name)\n",
    "    pivotedStatsTrainNoiseParm = Statistics.pivot_stats_df(dataframe=statsTrainNoiseParm, target_col_name=class_name)\n",
    "    pivotedStatsTrainNoise = pivotedStatsTrainNoiseParm.loc[:, ~pivotedStatsTrainNoiseParm.columns.duplicated()]\n",
    "    pivotedStatsTrainNoise=pivotedStatsTrainNoise.dropna()\n",
    "\n",
    "    trainX = pivotedStatsTrainNoise.drop(columns=class_name)\n",
    "    trainY = pivotedStatsTrainNoise[class_name].to_numpy()\n",
    "    \n",
    "    \n",
    "    # Get the Xt ready.\n",
    "    Xt = pd.concat([trainX, testX])\n",
    "    Xt = scaler.fit_transform(Xt)\n",
    "    Xt = pca.fit_transform(Xt)\n",
    "\n",
    "\n",
    "    # Perform PCA on our df and extract 2 components for visualization purposes.\n",
    "    testX = pca.fit_transform(X=testX)\n",
    "    trainX = pca.fit_transform(X=trainX)\n",
    "    X_train, X_test, y_train, y_test = scaler.fit_transform(trainX), scaler.fit_transform(testX), trainY, testY\n",
    "\n",
    "    h = 0.05\n",
    "    x_min, x_max = Xt[:, 0].min() - .5, Xt[:, 0].max() + .5\n",
    "    y_min, y_max = Xt[:, 1].min() - .5, Xt[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    # cm_bright = ListedColormap([\"#FF0000\", \"#FFFF00\", \"#00FF00\"])\n",
    "    cm = plt.cm.RdYlBu\n",
    "\n",
    "    # for i, name, model in zip(range(1, len(ax)), names, models):\n",
    "    model.fit(X_train, y_train)\n",
    "    hue = model.predict(X_test)\n",
    "    score = f1_score(y_test, hue, average='weighted')\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Reshape the array and then plot the contour plot.\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax[i].contourf(xx, yy, Z, cmap=cm, alpha=0.75)\n",
    "\n",
    "    # Now, we plot the points onto the contour and then map their\n",
    "    # colors according to the regions.\n",
    "    sns.scatterplot(x=X_test[:, 0], y=X_test[:, 1],\n",
    "                  hue=y_test, ax=ax[i], palette=['red', 'yellow', 'royalblue'])\n",
    "\n",
    "    ax[i].set_title(f'{title}, F-Score: {round(score, 2)}')\n",
    "    ax[i].get_legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig2, ax = plt.subplots(3, 1, figsize=(6, 14))\n",
    "fig2.set_facecolor('white')\n",
    "ax=ax.flatten()\n",
    "\n",
    "# Plot the original dataset.\n",
    "plot_original_dataset(ax, 'Species')\n",
    "\n",
    "# Plot contour for the original dataset.\n",
    "plot_contours_original(pivoted_original_stats, ax, \"2. Original Dataset Contour\", \"Species\", 1)\n",
    "\n",
    "# Plot contour for augmented dataset.\n",
    "plot_contour_augmented(testData, trainingData,\n",
    "                        ax, \"3. Augmented Dataset Contour\", 2, \"Species\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
