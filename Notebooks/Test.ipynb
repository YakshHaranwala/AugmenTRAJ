{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:21:31.086895949Z",
     "start_time": "2023-05-20T13:21:29.167192609Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from ptrail.core.Datasets import Datasets\n",
    "from ptrail.features.kinematic_features import KinematicFeatures\n",
    "from ptrail.preprocessing.statistics import Statistics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from src.augmentation.augment import Augmentation\n",
    "from src.selection.select import Selection\n",
    "from src.utils.general_utils import Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "seed = Utilities.generate_pi_seed(1)\n",
    "seed = next(seed)\n",
    "\n",
    "def augment_trajectories(dataset, to_augment, circle, class_col):\n",
    "    print(f\"Trajectories before augmentation: {len(dataset.traj_id.unique())}\")\n",
    "    dataset = Augmentation.augment_trajectories_with_randomly_generated_points(dataset,\n",
    "                                                                               ids_to_augment=to_augment,\n",
    "                                                                               circle=circle,\n",
    "                                                                               seed=seed)\n",
    "\n",
    "    for i in range(1, 10):\n",
    "        dataset = Augmentation.augment_trajectories_with_randomly_generated_points(dataset,\n",
    "                                                                                   ids_to_augment=to_augment,\n",
    "                                                                                   circle=circle,\n",
    "                                                                                   seed=seed)\n",
    "\n",
    "\n",
    "    print(f\"Trajectories after augmentation: {len(dataset.traj_id.unique())}\")\n",
    "    return Statistics.pivot_stats_df(dataframe=Statistics.generate_kinematic_stats(dataset, class_col), target_col_name=class_col)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:21:31.094405944Z",
     "start_time": "2023-05-20T13:21:31.092003625Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Dataset Facts ------------------------------\n",
      "\n",
      "Number of unique Trajectories in the data: 253\n",
      "Number of points in the data: 287136\n",
      "Dataset time range: 1196 days 22:51:45\n",
      "Datatype of the DataFrame: <class 'ptrail.core.TrajectoryDF.PTRAILDataFrame'>\n",
      "Dataset Bounding Box: (45.18896978643169, -118.61020848239596, 45.314545642992, -118.50455596234036)\n",
      "\n",
      "---------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and create the distance column in it which is necessary for Augmentation.\n",
    "final_results = []\n",
    "starkey_data = Datasets.load_starkey()\n",
    "ready_dataset = KinematicFeatures.create_distance_column(starkey_data).reset_index()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:21:36.566184855Z",
     "start_time": "2023-05-20T13:21:31.097344270Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "traj_ids = list(ready_dataset['traj_id'].unique())\n",
    "train_size = int(len(traj_ids) * 0.8)\n",
    "\n",
    "random.seed(seed)\n",
    "train_traj_ids = random.sample(traj_ids, train_size)\n",
    "test_traj_ids = []\n",
    "for t in traj_ids:\n",
    "    if t not in train_traj_ids:\n",
    "        test_traj_ids.append(t)\n",
    "\n",
    "training = ready_dataset.loc[ready_dataset.traj_id.isin(train_traj_ids)]\n",
    "testing = ready_dataset.loc[ready_dataset.traj_id.isin(test_traj_ids)]\n",
    "\n",
    "# Get the original train and test data ready.\n",
    "pivoted_train =Statistics.pivot_stats_df(\n",
    "    dataframe=Statistics.generate_kinematic_stats(training, 'Species'),\n",
    "    target_col_name='Species'\n",
    ")\n",
    "\n",
    "pivoted_test = Statistics.pivot_stats_df(\n",
    "    dataframe=Statistics.generate_kinematic_stats(testing, 'Species'),\n",
    "    target_col_name='Species'\n",
    ")\n",
    "\n",
    "x_test = pivoted_test.drop(columns=['Species'])\n",
    "y_test = pivoted_test['Species']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:22:09.970409963Z",
     "start_time": "2023-05-20T13:21:36.581179669Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 485 ms, sys: 71 Âµs, total: 486 ms\n",
      "Wall time: 486 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": "['1415, Base, RandomForest, 0.9803921568627451, 0.980125383486728']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "b_trainX = pivoted_train.drop(columns='Species')\n",
    "b_trainY = pivoted_train['Species']\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=256, random_state=seed)\n",
    "model.fit(b_trainX, b_trainY)\n",
    "\n",
    "predicted = model.predict(x_test)\n",
    "acc = accuracy_score(y_true=y_test, y_pred=predicted)\n",
    "f1 = f1_score(y_true=y_test, y_pred=predicted, average='weighted')\n",
    "final_results.append(f\"{seed}, Base, RandomForest, {acc}, {f1}\")\n",
    "final_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:22:10.470318705Z",
     "start_time": "2023-05-20T13:22:10.011124758Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.2 s, sys: 2.56 s, total: 33.7 s\n",
      "Wall time: 37.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Random selection.\n",
    "random_selected = Selection.select_randomly(training, seed=seed, k=0.2)\n",
    "\n",
    "# Proportional selection.\n",
    "proportional_selected = Selection.select_trajectories_proportionally(training, classification_col='Species', seed=seed, k=0.2)\n",
    "\n",
    "# Fewest selection.\n",
    "fewest_selected = Selection.select_with_fewest_points(training, k=0.2)\n",
    "\n",
    "# Representative Selection\n",
    "rep_selected = Selection.select_representative_trajectories(training, 'Species', closeness_cutoff=0.7, tolerance=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:22:47.612901Z",
     "start_time": "2023-05-20T13:22:10.482389614Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectories before augmentation: 202\n",
      "Trajectories after augmentation: 242\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_12034/1571822911.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     21\u001B[0m     \u001B[0;31m# Fit the model and predict.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 22\u001B[0;31m     \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mx_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0my_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     23\u001B[0m     \u001B[0mpred_vals\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mx_test\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/PreprocessingLibrary/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m    329\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0missparse\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    330\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 331\u001B[0;31m         X, y = self._validate_data(\n\u001B[0m\u001B[1;32m    332\u001B[0m             \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmulti_output\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maccept_sparse\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"csc\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mDTYPE\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    333\u001B[0m         )\n",
      "\u001B[0;32m~/anaconda3/envs/PreprocessingLibrary/lib/python3.8/site-packages/sklearn/base.py\u001B[0m in \u001B[0;36m_validate_data\u001B[0;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[1;32m    594\u001B[0m                 \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcheck_array\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"y\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mcheck_y_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    595\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 596\u001B[0;31m                 \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcheck_X_y\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mcheck_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    597\u001B[0m             \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    598\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/PreprocessingLibrary/lib/python3.8/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36mcheck_X_y\u001B[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[1;32m   1072\u001B[0m         )\n\u001B[1;32m   1073\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1074\u001B[0;31m     X = check_array(\n\u001B[0m\u001B[1;32m   1075\u001B[0m         \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1076\u001B[0m         \u001B[0maccept_sparse\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maccept_sparse\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/PreprocessingLibrary/lib/python3.8/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36mcheck_array\u001B[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[1;32m    897\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    898\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mforce_all_finite\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 899\u001B[0;31m             _assert_all_finite(\n\u001B[0m\u001B[1;32m    900\u001B[0m                 \u001B[0marray\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    901\u001B[0m                 \u001B[0minput_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minput_name\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/PreprocessingLibrary/lib/python3.8/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36m_assert_all_finite\u001B[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001B[0m\n\u001B[1;32m    144\u001B[0m                     \u001B[0;34m\"#estimators-that-handle-nan-values\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    145\u001B[0m                 )\n\u001B[0;32m--> 146\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmsg_err\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    147\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    148\u001B[0m     \u001B[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "select_strategies = [\n",
    "    'random', 'random', 'proportional', 'proportional',\n",
    "    'fewest', 'fewest', 'representation', 'representative'\n",
    "]\n",
    "augment_strategies = ['on', 'in', 'on', 'in', 'on', 'in', 'on', 'in']\n",
    "selected_traj = [\n",
    "    random_selected, random_selected, proportional_selected, proportional_selected,\n",
    "    fewest_selected, fewest_selected, rep_selected, rep_selected\n",
    "]\n",
    "\n",
    "\n",
    "for select, augment, traj in zip(select_strategies, augment_strategies, selected_traj):\n",
    "    # Create the model.\n",
    "    model = RandomForestClassifier(n_estimators=256, random_state=seed)\n",
    "\n",
    "    # Augment the trajectories and create the training set.\n",
    "    train = augment_trajectories(dataset=training, to_augment=traj, circle=augment, class_col='Species')\n",
    "    x_train = train.drop(columns=['Species'])\n",
    "    y_train = train['Species']\n",
    "\n",
    "    # Fit the model and predict.\n",
    "    model.fit(X=x_train, y=y_train)\n",
    "    pred_vals = model.predict(X=x_test)\n",
    "\n",
    "    # Calculate the accuracy and f1 score.\n",
    "    acc = accuracy_score(y_true=y_test, y_pred=pred_vals)\n",
    "    score = f1_score(y_true=y_test, y_pred=pred_vals, average='weighted')\n",
    "    final_results.append(f\"{seed}, {select}_{augment}, RandomForest, {acc}, {f1}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T13:24:48.964262673Z",
     "start_time": "2023-05-20T13:22:47.628020939Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_results"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
