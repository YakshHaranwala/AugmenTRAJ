{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.augmentation.augment import Augmentation\n",
    "from src.selection.select import Selection\n",
    "from src.utils.alter import Alter\n",
    "\n",
    "from ptrail.core.Datasets import Datasets\n",
    "from ptrail.core.TrajectoryDF import PTRAILDataFrame\n",
    "from ptrail.preprocessing.statistics import Statistics\n",
    "from ptrail.preprocessing.filters import Filters\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from random import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def trajectoryAugumentationProcedure(trajs, seed, n, k, pradius, model):\n",
    "    myRandom = Random(seed * (n * k * pradius))\n",
    "    #     Split data into training and testing\n",
    "    splits = Selection.select_randomly(trajs, myRandom, seed)\n",
    "    paramTestingDataSet = Filters.remove_duplicates(dataframe=trajs)\n",
    "\n",
    "    trainDataParm = paramTestingDataSet.loc[paramTestingDataSet.traj_id.isin(splits[\"train\"]) == True].dropna()\n",
    "    testDataParm = paramTestingDataSet.loc[paramTestingDataSet.traj_id.isin(splits[\"test\"]) == True].dropna()\n",
    "    #  Organize test data\n",
    "    testData = PTRAILDataFrame(data_set=testDataParm,\n",
    "                               latitude='lat',\n",
    "                               longitude='lon',\n",
    "                               datetime='DateTime',\n",
    "                               traj_id='traj_id')\n",
    "    #Calculate features for testing data\n",
    "    statsTestParm = Statistics.generate_kinematic_stats(dataframe=testData,\n",
    "                                                target_col_name='Species')\n",
    "    pivotedStatsTestParm = Statistics.pivot_stats_df(dataframe=statsTestParm,\n",
    "                                              target_col_name='Species')\n",
    "    #Remove duplicate columns\n",
    "    pivotedStatsTestParm = pivotedStatsTestParm.loc[:,~pivotedStatsTestParm.columns.duplicated()]\n",
    "\n",
    "    testParmX = pivotedStatsTestParm.drop(columns='Species')\n",
    "    testParmY = pivotedStatsTestParm['Species'].to_numpy()\n",
    "\n",
    "    # Organize training data\n",
    "    noiseTraj = trainDataParm['traj_id'].unique()\n",
    "    #Select n % of training data to augment\n",
    "    ### Here you are selecting without replacement, but the same trajectory could be\n",
    "    ### selected more than once according to the data augumentation theory I believe\n",
    "\n",
    "    #Using rnaomd.choices instead of sample has replacement\n",
    "    sampledTraj = myRandom.choices(sorted(noiseTraj), k=math.floor(n * len(noiseTraj)))\n",
    "    for traj in sampledTraj:\n",
    "        trajToChange = trainDataParm.loc[trainDataParm.traj_id == traj]\n",
    "        #Add noise to selected data then concatenate it to the training data\n",
    "        trajChanged = Augmentation.augment_trajectories_with_randomly_generated_points(trajToChange, pradius,\n",
    "                                                                                       k, 100, myRandom,)\n",
    "        trainDataParm = pd.concat([trainDataParm, trajChanged], ignore_index = True)\n",
    "\n",
    "    trainDataNoiseFilt = trainDataParm.filter([\"traj_id\", \"DateTime\",\"lat\", \"lon\", \"StarkeyTime\", \"GMDate\", \"GMTime\", \"LocDate\", \"LocTime\", \"RadNum\", \"Species\", \"UTME\", \"UTMN\", \"Year\", \"Grensunr\", \"Grensuns\", \"Obswt\"])\n",
    "\n",
    "\n",
    "\n",
    "    trainDataNoise = PTRAILDataFrame(data_set=trainDataNoiseFilt,\n",
    "                                            datetime='DateTime',\n",
    "                                            traj_id='traj_id',\n",
    "                                            latitude='lat',\n",
    "                                            longitude='lon')\n",
    "\n",
    "    #Calculate features of training data\n",
    "    statsTrainNoiseParm = Statistics.generate_kinematic_stats(dataframe=trainDataNoise,\n",
    "                                                        target_col_name='Species')\n",
    "\n",
    "    pivotedStatsTrainNoiseParm = Statistics.pivot_stats_df(dataframe=statsTrainNoiseParm,\n",
    "                                                      target_col_name='Species')\n",
    "\n",
    "    pivotedStatsTrainNoise = pivotedStatsTrainNoiseParm.loc[:,~pivotedStatsTrainNoiseParm.columns.duplicated()]\n",
    "\n",
    "    pivotedStatsTrainNoise=pivotedStatsTrainNoise.dropna()\n",
    "\n",
    "    trainParmX = pivotedStatsTrainNoise.drop(columns='Species')\n",
    "    trainParmY = pivotedStatsTrainNoise['Species'].to_numpy()\n",
    "\n",
    "    #Test model classification\n",
    "#     rf_model = RandomForestClassifier(random_state=seed)\n",
    "    model.fit(trainParmX, trainParmY)\n",
    "    test_predict = model.predict(testParmX)\n",
    "    performance_val = f1_score(testParmY, test_predict, average='weighted')\n",
    "    print(f\"Current run: k={k}, pradius={pradius}, n={n}, fscore={performance_val}, seed={seed}\")\n",
    "    return str(f\"{n},{k},{pradius},{performance_val}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Dataset Facts ------------------------------\n",
      "\n",
      "Number of unique Trajectories in the data: 253\n",
      "Number of points in the data: 287136\n",
      "Dataset time range: 1196 days 22:51:45\n",
      "Datatype of the DataFrame: <class 'ptrail.core.TrajectoryDF.PTRAILDataFrame'>\n",
      "Dataset Bounding Box: (45.18896978643169, -118.61020848239596, 45.314545642992, -118.50455596234036)\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "253 183\n",
      "252 167\n",
      "251 166\n",
      "250 88\n",
      "249 242\n",
      "248 55\n",
      "247 118\n",
      "246 240\n",
      "245 203\n",
      "244 163\n",
      "243 160\n",
      "242 214\n",
      "241 223\n",
      "240 55\n",
      "239 15\n",
      "238 69\n",
      "237 84\n",
      "236 221\n",
      "235 108\n",
      "234 172\n",
      "233 56\n",
      "232 4\n",
      "231 46\n",
      "230 28\n",
      "229 214\n",
      "228 26\n",
      "227 69\n",
      "226 122\n",
      "225 98\n",
      "224 151\n",
      "223 61\n",
      "222 201\n",
      "221 117\n",
      "220 84\n",
      "219 48\n",
      "218 47\n",
      "217 112\n",
      "216 15\n",
      "215 57\n",
      "214 22\n",
      "213 144\n",
      "212 3\n",
      "211 61\n",
      "210 98\n",
      "209 30\n",
      "208 135\n",
      "207 44\n",
      "206 34\n",
      "205 129\n",
      "204 130\n",
      "203 178\n",
      "202 2\n",
      "201 89\n",
      "200 173\n",
      "199 114\n",
      "198 29\n",
      "197 181\n",
      "196 60\n",
      "195 17\n",
      "194 144\n",
      "193 160\n",
      "192 182\n",
      "191 186\n",
      "190 0\n",
      "189 161\n",
      "188 161\n",
      "187 161\n",
      "186 177\n",
      "185 45\n",
      "184 101\n",
      "183 38\n",
      "182 128\n",
      "181 24\n",
      "180 1\n",
      "179 37\n",
      "178 124\n",
      "177 34\n",
      "176 140\n",
      "175 112\n",
      "174 73\n",
      "173 142\n",
      "172 158\n",
      "171 20\n",
      "170 15\n",
      "169 128\n",
      "168 102\n",
      "167 118\n",
      "166 32\n",
      "165 120\n",
      "164 128\n",
      "163 136\n",
      "162 128\n",
      "161 90\n",
      "160 118\n",
      "159 31\n",
      "158 71\n",
      "157 136\n",
      "156 27\n",
      "155 41\n",
      "154 63\n",
      "153 122\n",
      "152 128\n",
      "151 103\n",
      "150 28\n",
      "149 89\n",
      "148 54\n",
      "147 126\n",
      "146 95\n",
      "145 125\n",
      "144 128\n",
      "143 26\n",
      "142 137\n",
      "141 45\n",
      "140 36\n",
      "139 66\n",
      "138 54\n",
      "137 71\n",
      "136 10\n",
      "135 79\n",
      "134 70\n",
      "133 58\n",
      "132 14\n",
      "131 8\n",
      "130 10\n",
      "129 103\n",
      "128 104\n",
      "127 113\n",
      "126 66\n",
      "125 18\n",
      "124 59\n",
      "123 87\n",
      "122 113\n",
      "121 45\n",
      "120 51\n",
      "119 26\n",
      "118 96\n",
      "117 44\n",
      "116 20\n",
      "115 14\n",
      "114 16\n",
      "113 77\n",
      "112 41\n",
      "111 103\n",
      "110 13\n",
      "109 50\n",
      "108 77\n",
      "107 57\n",
      "106 78\n",
      "105 2\n",
      "104 101\n",
      "103 89\n",
      "102 77\n",
      "101 31\n",
      "100 60\n",
      "99 31\n",
      "98 55\n",
      "97 46\n",
      "96 39\n",
      "95 85\n",
      "94 93\n",
      "93 20\n",
      "92 86\n",
      "91 65\n",
      "90 84\n",
      "89 66\n",
      "88 46\n",
      "87 24\n",
      "86 1\n",
      "85 16\n",
      "84 31\n",
      "83 21\n",
      "82 35\n",
      "81 21\n",
      "80 2\n",
      "79 53\n",
      "78 70\n",
      "77 57\n",
      "76 56\n",
      "75 33\n",
      "74 10\n",
      "73 59\n",
      "72 42\n",
      "71 45\n",
      "70 24\n",
      "69 54\n",
      "68 8\n",
      "67 21\n",
      "66 20\n",
      "65 41\n",
      "64 60\n",
      "63 46\n",
      "62 37\n",
      "61 27\n",
      "60 51\n",
      "59 11\n",
      "58 54\n",
      "57 41\n",
      "56 16\n",
      "55 36\n",
      "54 32\n",
      "53 38\n",
      "52 0\n",
      "51 3\n",
      "50 40\n",
      "49 48\n",
      "48 40\n",
      "47 2\n",
      "46 28\n",
      "45 2\n",
      "44 11\n",
      "43 33\n",
      "42 20\n",
      "41 1\n",
      "40 24\n",
      "39 10\n",
      "38 2\n",
      "37 30\n",
      "36 10\n",
      "35 1\n",
      "34 3\n",
      "33 15\n",
      "32 24\n",
      "31 23\n",
      "30 6\n",
      "29 14\n",
      "28 25\n",
      "27 1\n",
      "26 11\n",
      "25 17\n",
      "24 19\n",
      "23 14\n",
      "22 19\n",
      "21 18\n",
      "20 7\n",
      "19 16\n",
      "18 9\n",
      "17 6\n",
      "16 13\n",
      "15 0\n",
      "14 10\n",
      "13 7\n",
      "12 0\n",
      "11 0\n",
      "10 9\n",
      "9 4\n",
      "8 1\n",
      "7 0\n",
      "6 3\n",
      "5 3\n",
      "4 0\n",
      "3 1\n",
      "2 0\n",
      "1 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty range for randrange()",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_58820/3810054665.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     16\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0ms\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mseeds\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m                 \u001B[0;31m# n = percentage of trajs to be augumented, k = percentage of points to be changed, and radius\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 18\u001B[0;31m                 print(trajectoryAugumentationProcedure(ready_dataset, s, n, k,\n\u001B[0m\u001B[1;32m     19\u001B[0m                                                        rad, RandomForestClassifier(random_state=s)))\n\u001B[1;32m     20\u001B[0m \u001B[0mtext_file\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_58820/1309730930.py\u001B[0m in \u001B[0;36mtrajectoryAugumentationProcedure\u001B[0;34m(trajs, seed, n, k, pradius, model)\u001B[0m\n\u001B[1;32m      2\u001B[0m     \u001B[0mmyRandom\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mRandom\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mseed\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mk\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mpradius\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0;31m#     Split data into training and testing\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m     \u001B[0msplits\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSelection\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect_randomly\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrajs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmyRandom\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mseed\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m     \u001B[0mparamTestingDataSet\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mFilters\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mremove_duplicates\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataframe\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtrajs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/AugmenTRAJ/src/selection/select.py\u001B[0m in \u001B[0;36mselect_randomly\u001B[0;34m(dataset, customRandom, test_split_per)\u001B[0m\n\u001B[1;32m     42\u001B[0m         \u001B[0mtestValues\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0munique_values_copy\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mtest_split_per\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 44\u001B[0;31m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0munique_values_copy\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcustomRandom\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrandrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0munique_values_copy\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     45\u001B[0m             \u001B[0mtestValues\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0munique_values_copy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcustomRandom\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrandrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0munique_values_copy\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/PreprocessingLibrary/lib/python3.8/random.py\u001B[0m in \u001B[0;36mrandrange\u001B[0;34m(self, start, stop, step, _int)\u001B[0m\n\u001B[1;32m    214\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mistart\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    215\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_randbelow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mistart\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 216\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"empty range for randrange()\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    217\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    218\u001B[0m         \u001B[0;31m# stop argument supplied.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: empty range for randrange()"
     ]
    }
   ],
   "source": [
    "# Import the dataset.\n",
    "dataset = Datasets.load_starkey()\n",
    "ready_dataset = Filters.remove_duplicates(dataframe=dataset)\n",
    "\n",
    "seeds = [14159,26535,89793,23846,26433,83279,50288,41971,69399,37510]\n",
    "n_vals = [.2, .3, .4, .5]\n",
    "k_vals = [.1, .2, .3, .4]\n",
    "rad_vals = [.001, .005, .01, .02]\n",
    "\n",
    "# Repeats 10 times (10 seeds) 80% training/ 20% testing\n",
    "\n",
    "text_file = open(\"results.txt\", \"w\")\n",
    "for n in n_vals:\n",
    "    for k in k_vals:\n",
    "        for rad in rad_vals:\n",
    "            for s in seeds:\n",
    "                # n = percentage of trajs to be augumented, k = percentage of points to be changed, and radius\n",
    "                print(trajectoryAugumentationProcedure(ready_dataset, s, n, k,\n",
    "                                                       rad, RandomForestClassifier(random_state=s)))\n",
    "text_file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}