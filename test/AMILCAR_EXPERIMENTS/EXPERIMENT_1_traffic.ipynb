{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-23T15:51:34.716066775Z",
     "start_time": "2023-08-23T15:51:33.248644097Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ptrail.core.TrajectoryDF import PTRAILDataFrame\n",
    "from ptrail.features.kinematic_features import KinematicFeatures\n",
    "from ptrail.core.Datasets import Datasets\n",
    "from ptrail.preprocessing.statistics import Statistics\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from TestUtils.test_utils import TestUtils\n",
    "from src.selection.select import Selection\n",
    "from src.utils.general_utils import Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Dataset Facts ------------------------------\n",
      "\n",
      "Number of unique Trajectories in the data: 125\n",
      "Number of points in the data: 44905\n",
      "Dataset time range: 0 days 00:00:59.900000\n",
      "Datatype of the DataFrame: <class 'ptrail.core.TrajectoryDF.PTRAILDataFrame'>\n",
      "Dataset Bounding Box: (34.7107417, 135.4640652, 34.7156517, 135.4702002)\n",
      "\n",
      "---------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                 vehicle_type  velocity  traffic_lane  \\\ntraj_id DateTime                                                        \n1371    1900-01-01 07:30:00.000             1      48.0             2   \n        1900-01-01 07:30:00.100             1      47.9             2   \n        1900-01-01 07:30:00.200             1      47.9             2   \n        1900-01-01 07:30:00.300             1      47.9             2   \n        1900-01-01 07:30:00.400             1      47.9             2   \n...                                       ...       ...           ...   \n3357    1900-01-01 07:30:59.500             1      27.7             1   \n        1900-01-01 07:30:59.600             1      27.7             1   \n        1900-01-01 07:30:59.700             1      29.0             1   \n        1900-01-01 07:30:59.800             1      30.3             1   \n        1900-01-01 07:30:59.900             1      31.0             1   \n\n                                        lon        lat  kilopost  \\\ntraj_id DateTime                                                   \n1371    1900-01-01 07:30:00.000  135.469950  34.710999    3539.5   \n        1900-01-01 07:30:00.100  135.469957  34.710991    3532.5   \n        1900-01-01 07:30:00.200  135.469963  34.710984    3532.5   \n        1900-01-01 07:30:00.300  135.469968  34.710979    3531.5   \n        1900-01-01 07:30:00.400  135.469972  34.710974    3530.8   \n...                                     ...        ...       ...   \n3357    1900-01-01 07:30:59.500  135.468970  34.712177    3697.6   \n        1900-01-01 07:30:59.600  135.468975  34.712172    3696.6   \n        1900-01-01 07:30:59.700  135.468981  34.712166    3695.6   \n        1900-01-01 07:30:59.800  135.468986  34.712160    3694.7   \n        1900-01-01 07:30:59.900  135.468992  34.712154    3693.7   \n\n                                 vehicle_length  detected_flag  Distance  \ntraj_id DateTime                                                          \n1371    1900-01-01 07:30:00.000             3.0              0       NaN  \n        1900-01-01 07:30:00.100             3.0              0  1.115504  \n        1900-01-01 07:30:00.200             3.0              0  0.939478  \n        1900-01-01 07:30:00.300             3.0              0  0.763477  \n        1900-01-01 07:30:00.400             3.0              0  0.596403  \n...                                         ...            ...       ...  \n3357    1900-01-01 07:30:59.500             3.5              0  0.737022  \n        1900-01-01 07:30:59.600             3.5              1  0.760166  \n        1900-01-01 07:30:59.700             3.5              1  0.788944  \n        1900-01-01 07:30:59.800             3.5              1  0.835374  \n        1900-01-01 07:30:59.900             3.5              1  0.873034  \n\n[44905 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>vehicle_type</th>\n      <th>velocity</th>\n      <th>traffic_lane</th>\n      <th>lon</th>\n      <th>lat</th>\n      <th>kilopost</th>\n      <th>vehicle_length</th>\n      <th>detected_flag</th>\n      <th>Distance</th>\n    </tr>\n    <tr>\n      <th>traj_id</th>\n      <th>DateTime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">1371</th>\n      <th>1900-01-01 07:30:00.000</th>\n      <td>1</td>\n      <td>48.0</td>\n      <td>2</td>\n      <td>135.469950</td>\n      <td>34.710999</td>\n      <td>3539.5</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1900-01-01 07:30:00.100</th>\n      <td>1</td>\n      <td>47.9</td>\n      <td>2</td>\n      <td>135.469957</td>\n      <td>34.710991</td>\n      <td>3532.5</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>1.115504</td>\n    </tr>\n    <tr>\n      <th>1900-01-01 07:30:00.200</th>\n      <td>1</td>\n      <td>47.9</td>\n      <td>2</td>\n      <td>135.469963</td>\n      <td>34.710984</td>\n      <td>3532.5</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>0.939478</td>\n    </tr>\n    <tr>\n      <th>1900-01-01 07:30:00.300</th>\n      <td>1</td>\n      <td>47.9</td>\n      <td>2</td>\n      <td>135.469968</td>\n      <td>34.710979</td>\n      <td>3531.5</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>0.763477</td>\n    </tr>\n    <tr>\n      <th>1900-01-01 07:30:00.400</th>\n      <td>1</td>\n      <td>47.9</td>\n      <td>2</td>\n      <td>135.469972</td>\n      <td>34.710974</td>\n      <td>3530.8</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>0.596403</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">3357</th>\n      <th>1900-01-01 07:30:59.500</th>\n      <td>1</td>\n      <td>27.7</td>\n      <td>1</td>\n      <td>135.468970</td>\n      <td>34.712177</td>\n      <td>3697.6</td>\n      <td>3.5</td>\n      <td>0</td>\n      <td>0.737022</td>\n    </tr>\n    <tr>\n      <th>1900-01-01 07:30:59.600</th>\n      <td>1</td>\n      <td>27.7</td>\n      <td>1</td>\n      <td>135.468975</td>\n      <td>34.712172</td>\n      <td>3696.6</td>\n      <td>3.5</td>\n      <td>1</td>\n      <td>0.760166</td>\n    </tr>\n    <tr>\n      <th>1900-01-01 07:30:59.700</th>\n      <td>1</td>\n      <td>29.0</td>\n      <td>1</td>\n      <td>135.468981</td>\n      <td>34.712166</td>\n      <td>3695.6</td>\n      <td>3.5</td>\n      <td>1</td>\n      <td>0.788944</td>\n    </tr>\n    <tr>\n      <th>1900-01-01 07:30:59.800</th>\n      <td>1</td>\n      <td>30.3</td>\n      <td>1</td>\n      <td>135.468986</td>\n      <td>34.712160</td>\n      <td>3694.7</td>\n      <td>3.5</td>\n      <td>1</td>\n      <td>0.835374</td>\n    </tr>\n    <tr>\n      <th>1900-01-01 07:30:59.900</th>\n      <td>1</td>\n      <td>31.0</td>\n      <td>1</td>\n      <td>135.468992</td>\n      <td>34.712154</td>\n      <td>3693.7</td>\n      <td>3.5</td>\n      <td>1</td>\n      <td>0.873034</td>\n    </tr>\n  </tbody>\n</table>\n<p>44905 rows Ã— 9 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_dataset = Datasets.load_traffic_data()\n",
    "ready_dataset = KinematicFeatures.create_distance_column(traffic_dataset)\n",
    "ready_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T15:51:35.581686398Z",
     "start_time": "2023-08-23T15:51:34.719633147Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[ 4.92387228e-01  7.18536678e-01  9.32588406e-01  1.05267739e+00\n  1.12301684e+00  2.56400882e-01  1.45972594e+00  8.80655430e-01\n  2.29382212e-01  1.72581043e+01  5.35462106e+01  1.30512590e+02\n  2.27400777e+02  2.81040356e+02  4.47378857e-01  3.15929210e+02\n  1.41170738e+02  9.60658859e+01  4.92387228e+00  7.18536678e+00\n  9.32588406e+00  1.05267739e+01  1.12301684e+01  2.56400882e+00\n  1.45972594e+01  8.80655430e+00  2.29382212e+00 -2.87333931e+00\n -1.21608600e+00  3.82173369e-06  1.43562354e+00  2.87308151e+00\n -1.63317788e+01  1.93345778e+01  2.15323457e-01  3.49793172e+00\n -2.32844295e+01 -1.42864621e+01  5.01291239e-04  1.25497167e+01\n  2.35355822e+01 -7.06313757e+01  7.94877890e+01  3.60991477e-02\n  2.01680585e+01  1.43157561e+02  1.43797956e+02  1.44373010e+02\n  1.44933656e+02  1.45606259e+02  1.38755035e+02  1.75911378e+02\n  1.44710432e+02  3.17959103e+00 -9.20773758e+00 -3.96959270e+00\n -1.64008451e-05  3.77245899e+00  8.15198286e+00 -1.40032450e+02\n  9.22651509e+01  3.56056766e-03  1.40780236e+01 -9.20773758e+00\n -3.96959270e+00 -1.64008451e-05  3.77245899e+00  8.15198286e+00\n -1.40032450e+02  9.22651509e+01  3.56056766e-03  1.40780236e+01].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 40\u001B[0m\n\u001B[1;32m     32\u001B[0m train_x, train_y \u001B[38;5;241m=\u001B[39m TestUtils\u001B[38;5;241m.\u001B[39maugment_trajectories_using_random_strategy(dataset\u001B[38;5;241m=\u001B[39mtrain,\n\u001B[1;32m     33\u001B[0m                                                                         percent_to_shake\u001B[38;5;241m=\u001B[39mshake,\n\u001B[1;32m     34\u001B[0m                                                                         ids_to_augment\u001B[38;5;241m=\u001B[39mselected,\n\u001B[1;32m     35\u001B[0m                                                                         circle\u001B[38;5;241m=\u001B[39mmethod,\n\u001B[1;32m     36\u001B[0m                                                                         n_augmentations\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m     37\u001B[0m                                                                         class_col\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvehicle_type\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# Calculate the distances.\u001B[39;00m\n\u001B[0;32m---> 40\u001B[0m mean, std \u001B[38;5;241m=\u001B[39m \u001B[43mTestUtils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfind_original_and_augmentation_pairs_and_calculate_differences\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mselected\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m f1_score \u001B[38;5;241m=\u001B[39m TestUtils\u001B[38;5;241m.\u001B[39mtrain_model_and_evaluate(model, scaler\u001B[38;5;241m.\u001B[39mfit_transform(train_x), train_y,\n\u001B[1;32m     42\u001B[0m                                               scaler\u001B[38;5;241m.\u001B[39mfit_transform(test_x), test_y, seed)\n\u001B[1;32m     43\u001B[0m distance_row\u001B[38;5;241m.\u001B[39mappend(mean)\n",
      "File \u001B[0;32m~/Desktop/AugmenTRAJ/test/TestUtils/test_utils.py:359\u001B[0m, in \u001B[0;36mTestUtils.find_original_and_augmentation_pairs_and_calculate_differences\u001B[0;34m(augmented_dataset, augmentation_targets)\u001B[0m\n\u001B[1;32m    356\u001B[0m \u001B[38;5;66;03m# # Now, for each augmented trajectory, find the Euclidean distance between the\u001B[39;00m\n\u001B[1;32m    357\u001B[0m \u001B[38;5;66;03m# # features of original trajectory and augmented trajectory and store it in a list.\u001B[39;00m\n\u001B[1;32m    358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m aug \u001B[38;5;129;01min\u001B[39;00m aug_features:\n\u001B[0;32m--> 359\u001B[0m     scaled_original \u001B[38;5;241m=\u001B[39m \u001B[43mscaler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43moriginal_features\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    360\u001B[0m     scaled_aug \u001B[38;5;241m=\u001B[39m scaler\u001B[38;5;241m.\u001B[39mfit_transform(aug)\n\u001B[1;32m    361\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOriginal: \u001B[39m\u001B[38;5;124m\"\u001B[39m, original_features)\n",
      "File \u001B[0;32m~/anaconda3/envs/PreprocessingLibrary/lib/python3.8/site-packages/sklearn/utils/_set_output.py:140\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[0;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 140\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    142\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[1;32m    143\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m    144\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    145\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[1;32m    146\u001B[0m         )\n",
      "File \u001B[0;32m~/anaconda3/envs/PreprocessingLibrary/lib/python3.8/site-packages/sklearn/base.py:878\u001B[0m, in \u001B[0;36mTransformerMixin.fit_transform\u001B[0;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[1;32m    874\u001B[0m \u001B[38;5;66;03m# non-optimized default implementation; override when a better\u001B[39;00m\n\u001B[1;32m    875\u001B[0m \u001B[38;5;66;03m# method is possible for a given clustering algorithm\u001B[39;00m\n\u001B[1;32m    876\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    877\u001B[0m     \u001B[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001B[39;00m\n\u001B[0;32m--> 878\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mtransform(X)\n\u001B[1;32m    879\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    880\u001B[0m     \u001B[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001B[39;00m\n\u001B[1;32m    881\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\u001B[38;5;241m.\u001B[39mtransform(X)\n",
      "File \u001B[0;32m~/anaconda3/envs/PreprocessingLibrary/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:427\u001B[0m, in \u001B[0;36mMinMaxScaler.fit\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    425\u001B[0m \u001B[38;5;66;03m# Reset internal state before fitting\u001B[39;00m\n\u001B[1;32m    426\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()\n\u001B[0;32m--> 427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpartial_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/PreprocessingLibrary/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:466\u001B[0m, in \u001B[0;36mMinMaxScaler.partial_fit\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    460\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    461\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMinMaxScaler does not support sparse input. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    462\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConsider using MaxAbsScaler instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    463\u001B[0m     )\n\u001B[1;32m    465\u001B[0m first_pass \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn_samples_seen_\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 466\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    467\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    468\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfirst_pass\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    469\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mFLOAT_DTYPES\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    470\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_all_finite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mallow-nan\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    471\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    473\u001B[0m data_min \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mnanmin(X, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m    474\u001B[0m data_max \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mnanmax(X, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/PreprocessingLibrary/lib/python3.8/site-packages/sklearn/base.py:565\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[0;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[1;32m    563\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation should be done on X, y or both.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    564\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[0;32m--> 565\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mX\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcheck_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    566\u001B[0m     out \u001B[38;5;241m=\u001B[39m X\n\u001B[1;32m    567\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n",
      "File \u001B[0;32m~/anaconda3/envs/PreprocessingLibrary/lib/python3.8/site-packages/sklearn/utils/validation.py:902\u001B[0m, in \u001B[0;36mcheck_array\u001B[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[1;32m    900\u001B[0m     \u001B[38;5;66;03m# If input is 1D raise error\u001B[39;00m\n\u001B[1;32m    901\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m array\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 902\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    903\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected 2D array, got 1D array instead:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124marray=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    904\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    905\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myour data has a single feature or array.reshape(1, -1) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    906\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mif it contains a single sample.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(array)\n\u001B[1;32m    907\u001B[0m         )\n\u001B[1;32m    909\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dtype_numeric \u001B[38;5;129;01mand\u001B[39;00m array\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mkind \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUSV\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    910\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    911\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnumeric\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is not compatible with arrays of bytes/strings.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    912\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConvert your data to numeric values explicitly instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    913\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: Expected 2D array, got 1D array instead:\narray=[ 4.92387228e-01  7.18536678e-01  9.32588406e-01  1.05267739e+00\n  1.12301684e+00  2.56400882e-01  1.45972594e+00  8.80655430e-01\n  2.29382212e-01  1.72581043e+01  5.35462106e+01  1.30512590e+02\n  2.27400777e+02  2.81040356e+02  4.47378857e-01  3.15929210e+02\n  1.41170738e+02  9.60658859e+01  4.92387228e+00  7.18536678e+00\n  9.32588406e+00  1.05267739e+01  1.12301684e+01  2.56400882e+00\n  1.45972594e+01  8.80655430e+00  2.29382212e+00 -2.87333931e+00\n -1.21608600e+00  3.82173369e-06  1.43562354e+00  2.87308151e+00\n -1.63317788e+01  1.93345778e+01  2.15323457e-01  3.49793172e+00\n -2.32844295e+01 -1.42864621e+01  5.01291239e-04  1.25497167e+01\n  2.35355822e+01 -7.06313757e+01  7.94877890e+01  3.60991477e-02\n  2.01680585e+01  1.43157561e+02  1.43797956e+02  1.44373010e+02\n  1.44933656e+02  1.45606259e+02  1.38755035e+02  1.75911378e+02\n  1.44710432e+02  3.17959103e+00 -9.20773758e+00 -3.96959270e+00\n -1.64008451e-05  3.77245899e+00  8.15198286e+00 -1.40032450e+02\n  9.22651509e+01  3.56056766e-03  1.40780236e+01 -9.20773758e+00\n -3.96959270e+00 -1.64008451e-05  3.77245899e+00  8.15198286e+00\n -1.40032450e+02  9.22651509e+01  3.56056766e-03  1.40780236e+01].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "seed_generator = Utilities.generate_pi_seed(1)\n",
    "seed_vals = [next(seed_generator) for i in range(1)]\n",
    "shake_percentages = [0.2, 0.4, 0.6]\n",
    "circle_methods = ['on', 'in']\n",
    "ml_models = [ExtraTreesClassifier(), GradientBoostingClassifier(), RandomForestClassifier()]\n",
    "scaler = MinMaxScaler((0, 1))\n",
    "\n",
    "distance_results = [\"on_20%_dist\", \"on_20%_std\", \"on_40%_dist\", \"on_40%_std\", \"on_60%_dist\", \"on_60%_std\",\n",
    "                    \"in_20%_dist\",\"in_20%_std\",\"in_40%_dist\",\"in_40%_std\",\"in_60%_dist\",\"in_60%_std\"]\n",
    "\n",
    "model_results = [\"model\", \"baseline\", \"in_20%_f1\", \"in_40%_f1\", \"in_60%_f1\", \"on_20%_f1\", \"on_40%_f1\", \"on_60%_f1\"]\n",
    "\n",
    "for seed in seed_vals:\n",
    "    # Intermediate lists for storing distance and model score values.\n",
    "    distance_row = []\n",
    "    model_row = []\n",
    "\n",
    "    # Set apart 20% data for testing that augmentation process will never see.\n",
    "    train, test_x, test_y = TestUtils.get_test_train_data(dataset=ready_dataset, seed_val=seed,\n",
    "                                                          class_col='vehicle_type', k=0.8)\n",
    "    for model in ml_models:\n",
    "        base_train_x, base_train_y = TestUtils.get_base_train_x_and_train_y(train, \"vehicle_type\")\n",
    "        model_row.append(model.__class__.__name__)\n",
    "        model_row.append(TestUtils.train_model_and_evaluate(model, scaler.fit_transform(base_train_x), base_train_y,\n",
    "                                                            scaler.fit_transform(test_x), test_y, seed))\n",
    "        for shake in shake_percentages:\n",
    "            for method in circle_methods:\n",
    "                # Randomly select 30% of trajectories to be augmented.\n",
    "                selected = Selection.select_randomly(train, seed, k=0.3)\n",
    "\n",
    "                # Augment the trajectories.\n",
    "                train_x, train_y = TestUtils.augment_trajectories_using_random_strategy(dataset=train,\n",
    "                                                                                        percent_to_shake=shake,\n",
    "                                                                                        ids_to_augment=selected,\n",
    "                                                                                        circle=method,\n",
    "                                                                                        n_augmentations=1,\n",
    "                                                                                        class_col=\"vehicle_type\")\n",
    "\n",
    "                # Calculate the distances.\n",
    "                mean, std = TestUtils.find_original_and_augmentation_pairs_and_calculate_differences(train_x, selected)\n",
    "                f1_score = TestUtils.train_model_and_evaluate(model, scaler.fit_transform(train_x), train_y,\n",
    "                                                              scaler.fit_transform(test_x), test_y, seed)\n",
    "                distance_row.append(mean)\n",
    "                distance_row.append(std)\n",
    "                model_row.append(f1_score)\n",
    "\n",
    "    print(distance_row)\n",
    "    print()\n",
    "    print(model_row)\n",
    "    print(\"----------------------------------------\")\n",
    "    # Append the intermediate lists to the final list.\n",
    "    distance_results.append(distance_row)\n",
    "    model_results.append(model_row)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T15:51:45.108173189Z",
     "start_time": "2023-08-23T15:51:35.598954604Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
